[TOC]

多线程
===

##**使用多线程实现tcp并发**

```
#server端
from threading import Thread,current_thread
from socket import *

def communicate(conn):
    print('线程2：%s'%current_thread().getName())  #查看线程的名字
    while True:
        try:
            data = conn.recv(1024)
            if not data: break
            conn.send(data.upper())
        except ConnectionResetError:
            break
    conn.close()
def server():
    print('线程1：%s' %current_thread().getName())
    server=socket(AF_INET,SOCK_STREAM)
    server.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)
    server.bind(('127.0.0.1',9090))
    server.listen(5)
    while True:
        conn,addr=server.accept()
        print(addr)
        t1=Thread(target=conmuicate,args=(conn,))   #启动一个线程，把建立的连接发送给这个线程去执行，实现并发
        t1.start()
    server.close()
if __name__=='__main__':
    server()

#client端
from socket import *
client=socket(AF_INET,SOCK_STREAM)
client.connect(('127.0.0.1',9090))
while True:
    cmd=input('>>>').strip()
    if not cmd: continue
    client.send(cmd.encode('utf-8'))
    data=client.recv(1024)
    print(data.decode('gbk'))
client.close()
```
##**开启线程的俩种方式**

```
#方法一：
from threading import Thread
import time
import random

def piao(name):
    print('%s is piaoing' %name)
    time.sleep(random.randint(1,2))
    print('%s is end' %name)

if __name__=='__main__':
    t1=Thread(target=piao,args=('lqx',))
    t1.start()
    print('主')


#方法二：使用class自定义线程名
from threading import Thread
import time
import random
class MyThread(Thread):
    def __init__(self,name):
        super(MyThread,self).__init__()   #super().__init__()
        self.name=name
    def run(self):
        print('%s is piaoing' %self.name)
        time.sleep(random.randint(1,2))
        print('%s is piaoing end ' %self.name)

if __name__=='__main__':
    t1=MyThread('lqx')
    t1.start()
    print('主')
```
##**进程和线程的俩种区别**

###区别一：启动数据快

```
from threading import Thread
from multiprocessing import Process
import time,random,os
def piao():
    print('%s is piaoing' %os.getpid(),os.getppid())
    # time.sleep(random.randint(1,2))
if __name__=='__main__':
    t1=Thread(target=piao)
    t2=Thread(target=piao)
    t3=Thread(target=piao)
    t4=Thread(target=piao)
    stat=time.time()
    # t1=Process(target=piao)
    # t2=Process(target=piao)
    # t3=Process(target=piao)
    # t4=Process(target=piao)
    l_t=[t1,t2,t3,t4]
    for t in l_t:t.start()
    print('主',os.getpid())
    stop=time.time()
    print('time %s' %(stop-stat))
```
###区别二：线程间资源共享，进程间资源独立

```
from multiprocessing import Process
from threading import Thread

n=100
def number():
    global n
    n=0

if __name__=='__main__':
    # p=Process(target=number)
    # p.start()
    # p.join()
    t=Thread(target=number)
    t.start()
    t.join()
    print('主：n=%s'%n)
#主：n=100
主：n=0
```
##**守护线程的使用**

```
#目的：开启守护线程后，主线程不会等待守护线程运行完，就会全部关闭
from threading import Thread
import time

def sayhi(name):
    print('>>>')
    time.sleep(1)
    print('%s say hello ' %name)

if __name__=='__main__':
    t=Thread(target=sayhi,args=('lqx',))
    t.daemon=True   # t.setDaemon(True)
    t.start()
    print('主线程')

#主进程运行完成后，会等待非守护线程运行完成，在这个等待过程中守护进程是不会关闭的
from threading import Thread
import time
def foo():
    print(123)
    time.sleep(1)
    print('end 123')
def bar():
    print(456)
    time.sleep(2)
    print('end 456')

if __name__=='__main__':
    t1=Thread(target=foo)
    t2=Thread(target=bar)
    t1.setDaemon(True)
    t1.start()
    t2.start()
    print('主')
```

#**线程的互斥锁**

```
#添加锁，会消耗代码的运行速度，但是保证了一个时间只能有一个线程去修改数据
from threading import Thread,Lock
import time
n=100

def task():
    global n
    # with mutex:   #添加锁的方法
    #     temp=n
    #     time.sleep(0.1)
    #     n=temp-1
    mutex.acquire()
    temp=n
    time.sleep(0.1)
    n=temp-1
    mutex.release()

if __name__=='__main__':
    start_time=time.time()
    mutex=Lock()
    t_l=[]
    for i in range(100):
        t=Thread(target=task)
        t_l.append(t)
        t.start()
    for t in t_l:
        t.join()
    stop_time=time.time()
    print('主',stop_time-start_time,n)
```
#**线程的GIL锁（解释器锁）**

```
#计算密集型：开多进程
from multiprocessing import Process
from threading import Thread
import os,time
def work():
    res=0
    for i in range(100000000):
        res*=i

if __name__=='__main__':
    l=[]
    start=time.time()
    for i in range(4):
        # p=Process(target=work)   #使用进程：run time is 15.462884426116943
        p=Thread(target=work)  #使用线程：run time is 36.300076484680176
        l.append(p)
        p.start()
    for p in l:
        p.join()
    stop=time.time()
    print('run time is %s ' %(stop-start))
```
```
#I/O密集型：多线程效率高
from multiprocessing import Process
from threading import Thread
import threading
import os,time
def work():
    time.sleep(2)
if __name__=='__main__':
    l=[]
    stat=time.time()
    for i in range(400):
        # p=Process(target=work)  #run time is 36.544090032577515
        p=Thread(target=work)  #run time is 2.0741186141967773
        l.append(p)
        p.start()
    for p in l:
        p.join()
    stop=time.time()
    print('run time is %s' %(stop-stat))
```
#**paramiko模块实现ssh登录**

```
import paramiko

#创建ssh对象
ssh=paramiko.SSHClient()
#允许连接不在know_hosts文件中的主机
ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
#连接服务器
ssh.connect(hostname='119.28.17.250',port=22,username='root',password='xxxxxx')

#执行命令：
while True:
    cmd=input('>>>').strip()
    if not cmd :continue
    if cmd=='exit':break
    stdin,stdout,stderr=ssh.exec_command(cmd)
    #获取命令结果
    result=stdout.read() + stderr.read()
    print(result.decode('utf-8'))

#关闭连接
ssh.close()
```
```
import paramiko

transport=paramiko.Transport(('119.28.17.250',22))
transport.connect(username='root',password='123,qwe.')

sftp=paramiko.SFTPClient.from_transport(transport)

sftp.put(r'D:\python20期课程\day9\02多线程\7GIL测试.py','/root/test.py')

sftp.get('/root/softether-vpnserver-v4.24-9651-beta-2017.10.23-linux-x64-64bit.tar.gz','D:\softether-vpnserver-v4.24-9651-beta-2017.10.23-linux-x64-64bit.tar.gz')

transport.close()
```
#**死锁和递归锁RLock**

>死锁：是指俩个或者俩个以上的进程或线程在执行过程中，因争夺资源而找出的一种互相等待的现象，若无外力作用，他们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程
```
from threading import Thread,RLock,Lock
import time
# mutexA=Lock()
# mutexB=Lock()
mutexA=mutexB=RLock()  #RLock锁可以解决死锁问题，只要锁的引用计数为0，大家就都可以抢了
class MyThread(Thread):
    def run(self):
        self.f1()
        self.f2()
    def f1(self):
        mutexA.acquire() #1
        print('%s 拿到了A锁' %self.name)
        mutexB.acquire() #2
        print('%s 拿到了B锁' %self.name)
        mutexB.release() # 1
        mutexA.release() # 0
        #此时引用计数都是0了，第一个线程也会跟着其他的线程一起去抢才能抢到锁
    def f2(self):
        mutexB.acquire()
        print('%s 拿到了B锁' %self.name)
        time.sleep(0.1)
        mutexA.acquire()
        print('%s 拿到了A锁' %self.name)
        mutexA.release()
        mutexB.release()

if __name__=='__main__':
    for i in range(10):
        t=MyThread()
        t.start()
```
#**信号量Semaphore**

>Semaphore管理一个内置的计数器，每当调用acquire()时内置计数器-1，调用release()时内置计数器+1，计数器不能小于0，当计数器为0时，acquire()将阻塞线程知道其他线程调用release()。
>**ps:信号量与进程池是完全不同的俩个概念，进程池Pool(4),最大只能产生4个进程，而且从头到尾都只是这4个进程，不会产生新的，而信号量是产生一对线程或者进程的**
```
from threading import Thread,Semaphore,current_thread
import time,random
#current_thread：
#current_thread().getName() 显示线程的名字

sm=Semaphore(5) #同时只有5个线程可以获得semaphore,即可以限制最大连接数为5

def task():
    with sm: #添加锁的另一种方法
        print('%s is laing' %current_thread().getName())
        time.sleep(random.randint(1,3))
if __name__ == '__main__':
    for i in range(20):
        t=Thread(target=task,)
        t.start()
```
#**Event事件**

>event.is_Set()     返回event的状态值；
>event.wait()     如果event.isSet()==False将阻塞线程；
>event.set()   设置event的状态值为True，所有阻塞池的线程激活进入就绪状态，等待操作系统调度；
>event.clear()   恢复event的状态值为False
```
#event事件就是：一个线程可以工作到某个时间点后，通知另一个线程开始运行
from threading import Thread,Event,current_thread
import time
# event.is_Set()：返回event的状态值；
# event.wait()：如果 event.isSet()==False将阻塞线程；
# event.set()： 设置event的状态值为True，所有阻塞池的线程激活进入就绪状态， 等待操作系统调度；
# event.clear()：恢复event的状态值为False

event=Event()
def check():
    print('checking MySQL...')
    # time.sleep(2)
    time.sleep(5)
    event.set()  #相等于把全局变量改为True

def conn():
    count=1
    while not event.is_set():  #相等于这里不是True，就进入循环
        if count >3 :
            raise TimeoutError('超时')
        print('%s try to connect mysql time %s ' %(current_thread().getName(),count))
        event.wait(1)  #阻塞1秒
        count+=1
    print('%s connected mysql' %current_thread().getName())

if __name__ == '__main__':
    t1=Thread(target=check)
    t2=Thread(target=conn)
    t3=Thread(target=conn)
    t4=Thread(target=conn)

    t1.start()
    t2.start()
    t3.start()
    t4.start()
```
#**定时器**

>指定一个线程多长时间后开始运行
```
from threading import Timer

def hello(name):
	print('hello,world %s' %name)
t=Timer(3,hello,args=('egon',))
t.start() 
```
#**线程的queue**

```python
import queue

#队列：先进先出
q=queue.Queue(3)

q.put(1)
q.put(2)
q.put(3)
# q.put(4) #队列满以后，会阻塞，不会抛出异常
# q.put_nowait(4)   # q.put(4,block=False)   #不等待,如果队列满了,会主动抛出异常,queue.Full
# q.put(4,block=True,timeout=3)  #等待3秒以后,主动抛出异常,queue.Full
print(q.get())
print(q.get())
print(q.get())
print(q.get())  #如果队列里面没有值，那么会阻塞

#堆栈：后进先出
q=queue.LifoQueue(3)
q.put(1)
q.put(2)
q.put(3)

print(q.get())
print(q.get())
print(q.get())

#优先级队列,值越低，优先级越大
q=queue.PriorityQueue(3)
q.put((10,'a'))
q.put((-3,'b'))
q.put((100,'c'))

print(q.get()[1])
print(q.get()[1])
print(q.get())
```
#**进程池ProcessPoolExecutor**

>提交任务的俩种方式：
>1、同步调用：提交完任务后，就在原地等待，等待任务执行完毕，拿到任务的返回值，才能继续下一行代码，导致程序串行执行
>2、异步调用+回调机制：提交完任务后，不在原地等待，任务一旦执行完毕就会触发回调函数的执行，程序是并发执行
>
>进程的执行状态：
>阻塞；同步调用，执行完任务后，在原地等待，也是阻塞状态
>非阻塞；
```python
1、介绍：
concurrent.futures：模块提供高度封装的异步调用接口
ThreadPoolExecutor：线程池，提供异步调用
ProcessPoolExecutor：进程池，提供异步调用
2、基本方法：
submit(fn,*args,**kwargs)  #异步提交任务
```
```python
#同步调用实例：
#concurrent.futures：模块提供高度封装的异步调用接口
#ThreadPoolExecutor：线程池，提供异步调用
#ProcessPoolExecutor：进程池，提供异步调用
#pool.shutdown(wait=True)  #shutdown意思是关闭池子，不能再次往池子中提交任务
#pool.submit(fn,*args,**kwargs) #异步提交任务
#result(timeout=None) #取得结果
#add_done_callback(fn) #回调函数

from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
import time,random,os

def task(n):
    print('%s is running' %os.getpid())
    time.sleep(random.randint(1,3))
    return n**2

def handle(res):
    # res=res.result() #另一种办法变成同步调用
    print('handle res %s' %res)

if __name__ == '__main__':
    pool=ProcessPoolExecutor(2) #设置池子的大小
    for i in range(5):
        res=pool.submit(task,i).result()  #运行一个进程，等待运行完毕，使用result()拿到结果
        # res = pool.submit(task, i) #另一种办法变成同步调用
        handle(res)
    pool.shutdown(wait=True)
    print('主')

#异步调用实例：
from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
import time,random,os

def task(n):
    print('%s is running' %os.getpid())
    time.sleep(random.randint(1,3))
    return n**2

def handle(res):
    res=res.result()
    print('handle res %s' %res)

if __name__ == '__main__':
    pool=ProcessPoolExecutor(2)
    for i in range(5):
        res = pool.submit(task, i)
        res.add_done_callback(handle)  #调用obj下面的方法，也就是回调函数
    pool.shutdown(wait=True)
    print('主')
```
#**线程池ThreadPoolExecutor**

```python
from concurrent.futures import ThreadPoolExecutor
from threading import current_thread
import requests
import time

def get(url):
    print('%s get %s' %(current_thread().getName(),url))
    response=requests.get(url)
    time.sleep(2)
    if response.status_code==200:
        return {'url':url,'content':response.text}

def parse(res):
    res=res.result()
    print('parse:[%s] res:[%s]' %(res['url'],len(res['content'])))

if __name__ == '__main__':
    pool=ThreadPoolExecutor(2)
    urls=[
        'https://www.baidu.com',
        'https://www.python.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
        'https://www.openstack.org',
    ]
    for url in urls:
        pool.submit(get,url).add_done_callback(parse)
    pool.shutdown(wait=True)
```